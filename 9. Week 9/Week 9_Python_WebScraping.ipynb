{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Web Scraping\n",
    "\n",
    "Web scraping is not always legal. Here are some important considerations:\n",
    "\n",
    "- **Respectful Requests**: Avoid overwhelming a website with scraping requests to prevent your IP address from being blocked.\n",
    "- **Check Permissions**: Always verify the website's terms of use before scraping. If an API is available, use it instead. Most websites prohibit commercial use of their data.\n",
    "- **Website Specifics**: Each website is unique and may change over time, so your scraping code may need updates or customization.\n",
    "\n",
    "### When to Scrape a Website:\n",
    "- **No API Available**: Scraping is useful when the information you need isn't accessible through an API.\n",
    "- **Anonymity**: Use a VPN if you want to scrape anonymously.\n",
    "\n",
    "### Web Scraping Practice:\n",
    "You can practice scraping on this sandbox: [http://toscrape.com/](http://toscrape.com/).\n",
    "\n",
    "Today, we'll start by scraping [Wikipedia](https://www.wikipedia.org/) because it is legal to scrape.\n",
    "\n",
    "This lesson is adapted from: [Complete Python 3 Bootcamp](https://github.com/Pierian-Data/Complete-Python-3-Bootcamp/blob/master/13-Web-Scraping/00-Guide-to-Web-Scraping.ipynb).\n",
    "\n",
    "### Installation:\n",
    "Make sure you have the following libraries installed:\n",
    "\n",
    "- **Requests**: `pip install requests`\n",
    "- **BeautifulSoup**: `pip install bs4`\n",
    "\n",
    "If you're using Anaconda:\n",
    "- **Requests**: `conda install requests`\n",
    "- **BeautifulSoup**: `conda install bs4`\n",
    "\n",
    "Or install directly in the notebook:\n",
    "```python\n",
    "!pip install requests\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the requests library to fetch web pages\n",
    "import requests\n",
    "\n",
    "# Fetch the Wikipedia page for \"Black Lives Matter\"\n",
    "response = requests.get(\"https://en.wikipedia.org/wiki/Black_Lives_Matter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the HTML content of the page\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response\n",
    "The `response.text` contains the HTML content of the Wikipedia page. This is the raw data we will parse using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup from bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Print the parsed HTML\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data\n",
    "Now that we have parsed the HTML, we can extract specific elements like titles, paragraphs, and links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the title of the page\n",
    "title = soup.title.string\n",
    "print(\"Page Title:\", title)\n",
    "\n",
    "# Extract the first paragraph\n",
    "first_paragraph = soup.find('p').text\n",
    "print(\"First Paragraph:\", first_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Links\n",
    "We can also extract all the links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all anchor tags (<a>) in the HTML\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Print the href attribute of each link\n",
    "for link in links:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This is a basic introduction to web scraping using Python. You can now fetch and parse web pages, extract specific data, and even save it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
